{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.models import resnet\n",
    "\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "import argparse\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mocotools import mocoutil \n",
    "\n",
    "gpu_info = !nvidia-smi -i 0\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Feature extraction')\n",
    "\n",
    "# General config\n",
    "parser.add_argument('--batch-size', default=64, type=int, metavar='N', help='mini-batch size')\n",
    "parser.add_argument('--pretrained_parameters', default= None, type=str)\n",
    "\n",
    "# moco specific configs:\n",
    "parser.add_argument('--symmetric', action='store_true', help='use a symmetric loss function that backprops to both crops')\n",
    "\n",
    "args = parser.parse_args('')  # running in ipynb\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.85, 0.7, 0.78], std=[0.15, 0.24, 0.2])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "img_file = {\n",
    "    '2x': '/path/to/my/tiles/training_patch280_2x_e70',\n",
    "    '5x': '/path/to/my/tiles/training5x_all/',\n",
    "    '20x': '/path/to/my/tiles/train20x_4000each/',\n",
    "    }\n",
    "\n",
    "checkpoint_file = {\n",
    "    '2x': '/path/to/my/checkpoints/2x_epoch200.pth',\n",
    "    '5x': '/path/to/my/checkpoints/5x_epoch160.pth',\n",
    "    '20x': '/path/to/my/checkpoints/20x_epoch200.pth'\n",
    "}\n",
    "\n",
    "def get_device(use_gpu):\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "Feature extraction from each magnification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_result_dir = '/path/to/features'\n",
    "\n",
    "for mgn in ['2x', '5x', '20x']:\n",
    "    model = mocoutil.ModelMoCo(dim=128,K=4096,m=0.99,T=0.1,arch='resnet18',\n",
    "        bn_splits=args.bn_splits,\n",
    "        symmetric=args.symmetric).cuda()\n",
    "    \n",
    "    dataset = mocoutil.ImageFolderWithPaths(img_file[mgn], transform = transform)\n",
    "    data_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                             num_workers=16, pin_memory=True)\n",
    "    \n",
    "    model_all = torch.load(checkpoint_file[mgn])\n",
    "    \n",
    "    print(f'Checkpoint loaded: Magnification: {mgn}, epoch: {model_all[\"epoch\"]}')\n",
    "    \n",
    "    model_state_dict = model_all['state_dict']\n",
    "    print(model.load_state_dict(model_state_dict))\n",
    "    \n",
    "    device = get_device(use_gpu=True) \n",
    "    print(f'device: {device}')\n",
    "    \n",
    "    feat, path = mocoutil.test(model.encoder_q, data_loader)\n",
    "          \n",
    "    feat_np = feat.to('cpu').detach().numpy().copy()\n",
    "          \n",
    "    # Normalize the value\n",
    "    std_scaler = preprocessing.StandardScaler()\n",
    "    feat_np_std = std_scaler.fit_transform(feat_np)\n",
    "          \n",
    "    \n",
    "    data_to_export = {'feat': feat_np_std, 'filename':path}\n",
    "    \n",
    "    target_dir = Path(f'{feat_result_dir}/{mgn}.pkl')\n",
    "    \n",
    "    joblib.dump(data_to_export, target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_result_csv_dir = '/path/to/cluster_results'\n",
    "\n",
    "for mgn in tqdm(['2x', '5x', '20x']):\n",
    "    cluster= {}\n",
    "    feat_path = Path(f'{feat_result_dir}/{mgn}.pkl')\n",
    "    feat = joblib.load(feat_path)\n",
    "    feature = feat['feat']\n",
    "    cluster['path'] = feat['filename']\n",
    "\n",
    "    for k in tqdm([3, 30, 50, 80, 100, 120, 150, 200]):\n",
    "        clustering = KMeans(n_clusters=k, random_state=300).fit(feature)\n",
    "        cluster[f'k{k}'] = clustering.labels_\n",
    "\n",
    "    pd.DataFrame(cluster).to_csv(f'{cls_result_csv_dir}/mgn{mgn}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_sample(df, n):\n",
    "    if df.shape[0] < n:\n",
    "        n_sample = df.shape[0]\n",
    "    else:\n",
    "        n_sample = n\n",
    "    \n",
    "    return df.sample(n_sample)\n",
    "\n",
    "def save_images(path, title, filename, fontsize = 8):   \n",
    "    row = 15\n",
    "    col = 8\n",
    "    fig = plt.figure(figsize=(15,30), dpi=100)\n",
    "    num = 0\n",
    "    \n",
    "    if len(path) == 0:\n",
    "        return None\n",
    "    \n",
    "    for i in range(1, row*col+1):\n",
    "        ax = fig.add_subplot(row, col, i)\n",
    "        try:\n",
    "            #print(path[i-1])\n",
    "            image = Image.open(path[i-1])\n",
    "            ax.imshow(image)\n",
    "            ax.set_title('', fontsize=fontsize)\n",
    "            ax.axis('off')\n",
    "        except IndexError:\n",
    "            return None\n",
    "        finally:\n",
    "            fig.savefig(filename)\n",
    "            plt.close()\n",
    "            \n",
    "def save_preview(cluster_list, title, path, cluster, n_cluster, target_dir):\n",
    "    df = pd.DataFrame({'cluster': cluster_list, 'title': title, 'path': path})\n",
    "    \n",
    "    df = df[df['cluster'] == cluster]\n",
    "    df = safe_sample(df, 120)\n",
    "    \n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    save_images(list(df['path']), list(df['title']), Path(target_dir).joinpath(f'cls{str(cluster).zfill(2)}.jpg'))\n",
    "            \n",
    "def save_preview_wrapper(args):\n",
    "    return save_preview(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previw_img_dir = '/path/to/preview'\n",
    "\n",
    "for mgn in tqdm(['2x', '5x', '20x']):\n",
    "    df = pd.read_csv(f'{cls_result_csv_dir}/mgn{mgn}.csv')\n",
    "    \n",
    "    for n_cluster in tqdm([3, 30, 50, 80, 100]):\n",
    "            \n",
    "        path = list(df['path'])\n",
    "        cluster = list(df[f'k{str(n_cluster)}'])\n",
    "        case_name = [Path(p).parent.name for p in list(df['path'])]\n",
    "        target = f'{previw_img_dir}/mgn{mgn}_k{str(n_cluster)}'\n",
    "        values = [(cluster, case_name, path, c, n_cluster, target) for c in range(n_cluster)]\n",
    "        \n",
    "        p = Pool(processes=cpu_count()-1)\n",
    "        p.map(save_preview_wrapper, values)\n",
    "        p.close()\n",
    "        p.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
